<section>
	<h2>Beam Beats</h2>
	<div class="row">
		<table>
			<tr>
				<td>Technologies</td>
				<td>openFrameworks, OpenCV, MIDI, Processing</td>
			</tr>
			<tr>
				<td>Source Code</td>
				<td>
					<a href="https://github.com/brendan-w/beam-beats">https://github.com/brendan-w/beam-beats</a>
					<br>
					<a href="https://github.com/nickMinnoe/BeamBeatsVis">https://github.com/nickMinnoe/BeamBeatsVis</a>
					<br>
					<a href="https://github.com/YouMeKim/beam-beats">https://github.com/YouMeKim/beam-beats</a>
				</td>
			</tr>
			<tr>
				<td>Collaborators</td>
				<td>
					<a href="http://courtneyboire.com/">Courtney Boire</a>
					<a href="http://www.ayechanpmoe.com/">Aye Chan Moe</a>
					<a href="http://www.nicoledombi.com/">Nicole Dombi</a>
					<a href="http://oxg7534.cias.rit.edu/#/">Olivia Grace</a>
					<a href="http://yumi-kim.me/">Yumi Kim</a>
					<a href="https://github.com/nickminnoe">Nick Minnoe</a>
					<a href="http://camrobinsondesign.com/">Cameron Robinson</a>
					<a href="http://maxwhitehead.com/">Max Whitehead</a>
				</td>
			</tr>
			<tr>
				<td>Role</td>
				<td>Lead Developer and Hardware Engineer</td>
			</tr>
			<tr>
				<td>License</td>
				<td>MIT</td>
			</tr>
		</table>
	</div>
	<div class="row">
		<div class="col">
			<p>
				Beam Beats is a collaborative musical instrument that generates data visualizations of the users compositions. Using four beams of light as an interface, users are encouraged to experiment with the sound of an electric guitar, and to take home their visualizations as keepsakes from the experience. Prototyped for the <a href="http://rockhall.com/">Rock &amp; Roll Hall of Fame</a>, the instrument is designed to provide the sense of reckelss creativity that has been so fundamental to the development of Rock &amp; Roll.
			</p>
		</div>
		<div class="col">
			<p>
				The instrument is operated by simply placing your hands in the beams. The vertical height of the user's hand corresponds to a six note pentatonic scale, and shaking horizontally will produce vibrato. This data is transmitted via <a href="https://en.wikipedia.org/wiki/MIDI">MIDI</a> to a synthesizer and our custom visualization engine.
			</p>
		</div>
	</div>
	<div class="row">
		<img src="/assets/projects/beambeats_beauty.jpg">
	</div>
	<div class="row">
		<div class="col"><img src="/assets/projects/beambeats_full.jpg"></div>
		<div class="col"><img src="/assets/projects/beambeats_hand.jpg"></div>
	</div>
</section>
<section>
	<h3>Hardware Process</h3>
	<div class="row">
		<div class="col">
			<p>
				After visiting the Rock Hall, our team was struck by the overwelming amount of content consumption, and the corresponding lack of content creation. With hundreds of feet of glass cases, users never get the opportunity to touch and play with an instrument. With this problem in mind, we focused on ceating a musically-reckless experience.
			</p>
			<p>
				Our team deliberated on various forms of musical instruments, from samplers to physical signal chains. We eventually landed on the idea of virtual guitar strings. The descision to use light as an interface came not only because it was cinematic, but because it offered us more expression from the user. Through basic gesture detection, a user's hand can control multiple aspects of the sound.
			</p>
			<p>
				After much experimentation, we settled on using digital projectors as the light sources for our beams. With some careful optics, a single projector can be made to power multiple beams, giving us full color and animation control.
			</p>
		</div>
		<div class="col">
			<img src="/assets/projects/beambeats_sketch.png">
			<p>sketch by Max Whitehead</p>
		</div>
	</div>
	<div class="row">
		<div class="col">
			<img src="/assets/projects/beambeats_prototype_1.jpg">
			<p>Our first prototype of the collimation optics. The projector's image is split vertically with mirrors, such that the left and right halves of the image control individual beams.</p>
		</div>
		<div class="col">
			<img src="/assets/projects/beambeats_prototype_2.jpg">
			<p>Each beam is collimated using a 6x12 theatrical Leko lens. For an even beam, it is also important to use short-throw projectors for their approximation of a point source.</p>
		</div>
	</div>
	<div class="row">
		<div class="col">
			<img src="/assets/projects/beambeats_hardware_1.jpg">
			<p>After taking careful measurements from our prototype, we constructed two enclosures to hold all of our optics. This allows us to create four interactive beams using just two projectors.</p>
		</div>
		<div class="col"><img src="/assets/projects/beambeats_hardware_2.jpg"></div>
	</div>
	<div class="row">
		<div class="col">
			<img src="/assets/projects/beambeats_adjust_1.jpg">
			<p>To allow for calibration, many of the optical elements were built to be adjustable. Here, you can see that the beam splitters are allowed to be rotated and translated.</p>
		</div>
		<div class="col">
			<img src="/assets/projects/beambeats_adjust_2.jpg">
			<p>Using two screws as adjusters (a variation on standard optical mounts), the main mirrors are aimable by Â±10 degrees in two axis. This allows us to perfectly align all four beams.</p>
		</div>
	</div>
</section>
<section>
	<h3>Hand Detection</h3>
	<div class="row">
		<div class="col">
			<p>
				Beam Beats uses two <a href="https://en.wikipedia.org/wiki/PlayStation_Eye">Play Station 3 Eye</a> cameras for its hand detection. Built specifically for computer vision, these cameras stream video at 60 frames per second, and provide programmatic controls for the on-board exposure and white-balance settings. The video from these cameras is processed with a computer vission system built with <a href="http://opencv.org/">OpenCV</a>. Using simple thresholding techniques, the application performs bright-object detection to find the user's hand(s) in each beams.
			</p>
			<p>
				To calibrate the instrument, these cameras must be trained to distinguish between the various beams. This can be done by using your hand to "paint" a mask of the beam's location. This mask is used to determine the location of each beam in the camera's view, and to help ignore other light sources. By fitting a minumum area rectangle to each mask, the application can calculate the extremes of each beam. The major axis of this rectangle is then used to compute the height of a user's hand.
			</p>
			<p>
				To lessen the influence of ambient room light and fog effects, the computer also performs background subtraction on the incoming video.
			</p>
		</div>
		<div class="col">
			<img src="/assets/projects/beambeats_masks.png">
		</div>
	</div>
</section>
<section>
	<h3>Visualizations</h3>
	<div class="row">
		<div class="col">
			<p>
				The music visualizations for Beam Beats are created in real time, from the same stream of MIDI data that is sent to the synthesizer. Notes are displayed as small squares on a polar plot, where radius is mapped to pitch, and angle is mapped to time. Each note is color coded to display which beam (and therefore, which person) played it. Users get one minute to perform music on the instrument, during which time, the visualization will complete one revloution, and create a finished image.
			</p>
			<p>
				When visualizations are completed, they are sent to a server, and are posted on our website. When done, users can look up their visualizations by ID number, and have them emailed as keepsakes. During a demo at <a href="http://www.rit.edu/imagine/">Imagine RIT</a>, we collected 327 visualizations in a 7 hour period. A handful of them are shown below:
			</p>
		</div>
		<div class="col">
			<img src="/assets/projects/beambeats_user.jpg">
		</div>
	</div>
	<div class="row">
		<img src="/assets/projects/beambeats_vis.png">
	</div>
</section>
